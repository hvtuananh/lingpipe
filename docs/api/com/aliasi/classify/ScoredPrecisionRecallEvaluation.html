<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!--NewPage-->
<HTML>
<HEAD>
<!-- Generated by javadoc (build 1.6.0_14) on Thu Jun 23 19:27:25 EDT 2011 -->
<TITLE>
ScoredPrecisionRecallEvaluation (LingPipe API)
</TITLE>

<META NAME="date" CONTENT="2011-06-23">

<LINK REL ="stylesheet" TYPE="text/css" HREF="../../../stylesheet.css" TITLE="Style">

<SCRIPT type="text/javascript">
function windowTitle()
{
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="ScoredPrecisionRecallEvaluation (LingPipe API)";
    }
}
</SCRIPT>
<NOSCRIPT>
</NOSCRIPT>

</HEAD>

<BODY BGCOLOR="white" onload="windowTitle();">
<HR>


<!-- ========= START OF TOP NAVBAR ======= -->
<A NAME="navbar_top"><!-- --></A>
<A HREF="#skip-navbar_top" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_top_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../com/aliasi/classify/ScoredClassifierEvaluator.html" title="class in com.aliasi.classify"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../com/aliasi/classify/TfIdfClassifierTrainer.html" title="class in com.aliasi.classify"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../index.html?com/aliasi/classify/ScoredPrecisionRecallEvaluation.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="ScoredPrecisionRecallEvaluation.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_top"></A>
<!-- ========= END OF TOP NAVBAR ========= -->

<HR>
<!-- ======== START OF CLASS DATA ======== -->
<H2>
<FONT SIZE="-1">
com.aliasi.classify</FONT>
<BR>
Class ScoredPrecisionRecallEvaluation</H2>
<PRE>
<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">java.lang.Object</A>
  <IMG SRC="../../../resources/inherit.gif" ALT="extended by "><B>com.aliasi.classify.ScoredPrecisionRecallEvaluation</B>
</PRE>
<HR>
<DL>
<DT><PRE>public class <B>ScoredPrecisionRecallEvaluation</B><DT>extends <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">Object</A></DL>
</PRE>

<P>
A <code>ScoredPrecisionRecallEvaluation</code> provides an
 evaluation based on the precision-recall operating points and
 sensitivity-specificity operating points.  The unscored
 precision-recall evaluation class is <A HREF="../../../com/aliasi/classify/PrecisionRecallEvaluation.html" title="class in com.aliasi.classify"><CODE>PrecisionRecallEvaluation</CODE></A>.

 <h3>Construction and Population</h3>

 <p>There is a single no-arg constructor <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#ScoredPrecisionRecallEvaluation()"><CODE>ScoredPrecisionRecallEvaluation()</CODE></A>.  

 <p>The method <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#addCase(boolean, double)"><CODE>addCase(boolean,double)</CODE></A> is used to populate
 the evaluation, with the first argument representing whether the
 response was correct and the second the score that was assigned.

 <h4>Missing Cases</h4>

 <p>If there are positive reference cases that are not added through
 <code>addCase()</code>, the total number of such cases should be added
 using the method <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#addMisses(int)"><CODE>addMisses(int)</CODE></A>.  This method effectively
 increments the number of reference positive cases used to compute
 recall values.

 <p>If there are negative reference cases that are not dealt with
 through <code>addCase()</code>, the method <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#addNegativeMisses(int)"><CODE>addNegativeMisses(int)</CODE></A> should be called with the total number of
 such cases as an argument.  This method increments the number of 
 reference engative cases used to compute specificity values.

 <h3>Example</h3>

 <P>By way of example, consider the following table of cases, all of
 which involve positive responses.  The cases are in rank order, but
 may be added in any order.  

 <blockquote>
 <table border="1" cellpadding="5">
 <tr><td><i>Rank</i></td>
     <td><i>Score</i></td>
     <td><i>Correct</i></td>
     <td><i>TP</i></td>
     <td><i>TN</i></td>
     <td><i>FP</i></td>
     <td><i>FN</i></td>
     <td><i>Rec</i></td>
     <td><i>Prec</i></td>
     <td><i>Spec</i></td>
     <td><i>F Meas</i></td></tr>
 <tr><td>(-1)</td><td>n/a</td><td>n/a</td>
     <td>0</td> <td>6</td> <td>0</td> <td>5</td>
     <td>0.00</td>
     <td bgcolor="yellow">1.00</td>
     <td bgcolor="orange">1.00</td>
     <td>0.00</td></tr>
 <tr><td colspan="11"> </td></tr>
 <tr><td>0</td><td>-1.21</td><td>no</td>
     <td>0</td> <td>5</td> <td>1</td> <td>5</td>
     <td>0.00</td>
     <td>0.00</td>
     <td>0.83</td>
     <td>0.00</td></tr>
 <tr bgcolor="#CCCCFF"><td>1</td><td>-1.27</td><td>yes</td>
     <td>1</td> <td>5</td> <td>1</td> <td>4</td>
     <td>0.20</td>
     <td>0.50</td>
     <td bgcolor="orange">0.83</td>
     <td>0.29</td></tr>
 <tr><td>2</td><td>-1.39</td><td>no</td>
     <td>1</td> <td>4</td> <td>2</td> <td>4</td>
     <td>0.20</td>
     <td>0.33</td>
     <td>0.67</td>
     <td>0.25</td></tr>
 <tr bgcolor="#CCCCFF"><td>3</td><td>-1.47</td><td>yes</td>
     <td>2</td> <td>4</td> <td>2</td> <td>3</td>
     <td>0.40</td>
     <td>0.50</td>
     <td>0.67</td>
     <td>0.44</td></tr>
 <tr bgcolor="#CCCCFF"><td>4</td><td>-1.60</td><td>yes</td>
     <td>3</td> <td>4</td> <td>2</td> <td>2</td>
     <td>0.60</td>
     <td bgcolor="yellow">0.60</td>
     <td bgcolor="orange">0.67</td>
     <td bgcolor="pink">0.60</td></tr>
 <tr><td>5</td><td>-1.65</td><td>no</td>
     <td>3</td> <td>3</td> <td>3</td> <td>2</td>
     <td>0.60</td>
     <td>0.50</td>
     <td>0.50</td>
     <td>0.55</td></tr>
 <tr><td>6</td><td>-1.79</td><td>no</td>
     <td>3</td> <td>2</td> <td>4</td> <td>2</td>
     <td>0.60</td>
     <td>0.43</td>
     <td>0.33</td>
     <td>0.50</td></tr>
 <tr><td>7</td><td>-1.80</td><td>no</td>
     <td>3</td> <td>1</td> <td>5</td> <td>2</td>
     <td>0.60</td>
     <td>0.38</td>
     <td>0.17</td>
     <td>0.47</td></tr>
 <tr bgcolor="#CCCCFF"><td>8</td><td>-2.01</td><td>yes</td>
     <td>4</td> <td>1</td> <td>5</td> <td>1</td>
     <td>0.80</td>
     <td bgcolor="yellow">0.44</td>
     <td bgcolor="orange">0.17</td>
     <td>0.53</td></tr>
 <tr><td>9</td><td>-3.70</td><td>no</td>
     <td>4</td> <td>0</td> <td>6</td> <td>1</td>
     <td>0.80</td>
     <td>0.40</td>
     <td>0.00</td>
     <td>0.53</td></tr>
 <tr><td colspan="11"> </td></tr>
 <tr><td>?</td><td>n/a</td><td>yes</td>
     <td>5</td> <td>0</td> <td>6</td> <td>0</td>
     <td>1.00</td>
     <td bgcolor="yellow">0.00</td>
     <td bgcolor="orange">0.00</td>
     <td>0.00</td></tr>
 </table>
 </blockquote>

 The first line, which is separated, indicates the values before any
 results have been returned.  There's no score corresponding to this
 operating point, and given that it doesn't correspond to a result,
 correctness is not applicable.  It has zero recall, one
 specificity, and one precision (letting zero divided by zero be one
 here).

 <p>The next lines, listed as ranks 0 to 9, correspond to calls to
 <code>addCase()</code> with the specified score and correctness.  For
 each of these lines, we list the corresponding number of true
 positives (TP), true negatives (TN), false positives (FP), and
 false negatives (FN).  These are followed by recall, precision and
 specificity (aka rejection recall).  See the class documentation
 for <A HREF="../../../com/aliasi/classify/PrecisionRecallEvaluation.html" title="class in com.aliasi.classify"><CODE>PrecisionRecallEvaluation</CODE></A> for definitions of these
 values in terms of the TP, TN, FP, and FN counts.

 <p>There are five positive reference cases (blue backgrounds) and
 six negative reference cases (clear backgrounds) in this diagram.
 The yellow precision values and orange specificity values are used
 for interpolated curves.

 <h3>Precision-Recall Curves</h3>
 
 <P>The pairs of precision/recall values form the basis for the
 precision-recall curve returned by <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#prCurve(boolean)"><CODE>prCurve(boolean)</CODE></A>, with
 the argument indicating whether to perform precision interpolation.
 For the above graph, the uninterpolated precision-recall curve is:

 <blockquote><pre>
 <b>prCurve</b>(false) = {
     { 0.00, 1.00 },
     { 0.20, 0.50 },
     { 0.20, 0.33 },
     { 0.40, 0.50 },
     { 0.60, 0.60 },
     { 0.60, 0.50 },
     { 0.60, 0.43 },
     { 0.60, 0.38 },
     { 0.60, 0.38 },
     { 0.80, 0.44 },
     { 0.80, 0.40 },
     { 1.00, 0.00 }
 }</pre></blockquote>
 
 Typically, a form of interpolation is performed that sets the
 precision for a given recall value to the maximum of the precision
 at the curent or greater recall value.  This pushes the yellow
 precision values up the graph.  At the same time, we only return
 values that correspond to jumps in recall, corresponding to ranks
 at which true positives were returned.  For the example above,
 the result is

 <blockquote><pre>
 <b>prCurve</b>(true) = {
     { 0.00, 1.00 },
     { 0.20, 0.60 },
     { 0.40, 0.60 },
     { 0.60, 0.60 },
     { 0.80, 0.44 },
     { 1.00, 0.00 }
 }</pre></blockquote>

 For convenience, the evaluation always adds the two limit points,
 one with precision 0 and recall 1, and one with precision 1 and
 recall 0.  These operating points are always achievable, the first
 by returning every possible answer, and the second by returning no
 answers.  

 <h3>ROC Curves</h3>

 Another popular graph for visualizing classification results is the
 receiver operating characteristic (ROC) curve, which plots
 sensitivity (a.k.a. recall) versus one minus specificity
 (a.k.a. one minus rejection recall).  Because specificity is
 accuracy on negative cases and sensitivity accuracy on positive
 cases, these graphs are fairly easy to interpret.  The
 precision-recall curve, on the othe hand, does not consider true
 negative (TN) counts.
 
 <p>The ROC curve is returned by the method <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#rocCurve(boolean)"><CODE>rocCurve(boolean)</CODE></A> with the boolean parameter again indicating
 whether to perform precision interpolation.  For the above graph,
 the result is:

 <blockquote><pre>
 <b>rocCurve</b>(false) = {
     { 1 - 1.00, 0.00 },
     { 1 - 0.83, 0.00 },
     { 1 - 0.83, 0.20 },
     { 1 - 0.67, 0.20 },
     { 1 - 0.67, 0.40 },
     { 1 - 0.67, 0.60 },
     { 1 - 0.50, 0.60 },
     { 1 - 0.33, 0.60 },
     { 1 - 0.17, 0.60 },
     { 1 - 0.17, 0.80 },
     { 1 - 0.00, 0.80 },
     { 1 - 0.00, 1.00 }
 }</pre></blockquote>

 Interpolation works exactly the same way as for the precision-recall
 curves, but based on specificity rather than precsion.

 <blockquote><pre>
 <b>rocCurve</b>(true) = {
     { 1 - 1.00, 0.00 },
     { 1 - 0.83, 0.20 },
     { 1 - 0.67, 0.60 },
     { 1 - 0.50, 0.60 },
     { 1 - 0.33, 0.60 },
     { 1 - 0.17, 0.80 },
     { 1 - 0.00, 1.00 }
 }</pre></blockquote>


 <h3>Precision at <i>N</i></h3>

 <p>In some information extraction or retrieval tasks, a system
 might only return a fixed number of examples to a user.  To
 evaluate the result of such truncated result sets, it is common to
 report the precision after <i>N</i> returned results.  The counting
 starts from one rather than zero for returned results, but we fill in
 a limiting value of 1.0 for precision at 0.  In our running
 example, we have

 <pre>
      <b>precisionAt</b>(0) = 1.0
      <b>precisionAt</b>(1) = 0.0
      <b>precisionAt</b>(5) = 0.6
      <b>precisionAt</b>(10) = 0.4
      <b>precisionAt</b>(20) = 0.2
      <b>precisionAt</b>(100) = 0.04</pre>

 The return value for a rank greater than the number of cases added
 will be calculated assuming all other results are errors.
 
 <h3>Reciprocal Rank</h3>

 For information extraction tasks, one result is often enough to
 satisfy an information need.  A popular measure to characterize
 this situation is reciprocal rank.  The reciprocal rank is defined
 to be <code>1/<i>M</i></code>, where <code><i>M</i></code> is the
 rank (counting from 1) of the first true positive return.  In our
 running example, the first result is a false positive and the
 second a true positive, so reciprocal rank is

 <pre>
      <b>reciprocalRank()</b>() = 0.5</pre>

 Note that this measure emphasizes differences in early ranks
 much more than later ones.  For instance, the reciprocal rank
 for a system returning a correct result first is 1/1, but
 for one returning it second, it's 1/2, and for one returning
 the first true positive at rank 10, it's 1/10.  The difference
 between rank 1 and 2 is greater than that between 2 and 10. 

 <h3>R Precision</h3>

 The R precision is defined as the precision for the first R
 results, where R is the number of reference positive cases.  If
 there are not enough results, the value returned is calculated by
 assuming all the non-added results are errors.  

 <p>For the running example, R precision is

 <blockquote><pre>
 <b>rPrecision</b>() = 0.6</pre></blockquote>

 R precision will always be at a point where precision equals
 recall.  It is also known as the precision-recall break-even point
 (BEP), and for convenience, there is a method of that name,

 <blockquote><pre>
 <b>prBreakevenPoint</b>() = rPrecision() = 0.6</pre></blockquote>

 <h3>Maximum F Measure</h3>

 Another commonly reported statistic that may be calculated from the
 precisino-recall curve is the maximum F measure (see <A HREF="../../../com/aliasi/classify/PrecisionRecallEvaluation.html#fMeasure(double, double, double)"><CODE>PrecisionRecallEvaluation.fMeasure(double,double,double)</CODE></A> for a
 definition of F measure).  The result is the maximum F measure
 value achieved at any position on the curve.  For our example, this
 arises at

 <blockquote><pre>
 maximumFMeasure() = 0.6</pre></blockquote>

 <p>In general, the maximum F measure may occur at a point
 other than the precision-recall break-even point.

 <h3>Averaged Results</h3>

 If there is more than one classifier or information extractor
 being evaluated, it is common to report averages of several
 of the statistics reported by this class.  LingPipe does not compute
 these values, but they are easy to calculate by accumulating
 results for individual ranked precision-recall evaluations.

 <p>The average across multiple evaluations of average precision is
 somewhat misleadingly called mean average precision (MAP) [it should
 be average average precision, because averages are over finite
 samples and means are properties of distributions].

 <p>The eleven-point precision-recall curves, reciprocal rank, and R
 precision are also popular targets for reporting averaged results.

 <h3>References</h3>

 For texts on ROC and PR evaluations, see the following.

 <ul> 
 
 <li>Wikipedia. <a
 href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">Receiver
 Operating Characteristic</a>.

 <li>Lasko, Thomas A., Jui G. Bhagwat, Kelly H. Zou, and Lucila Ohno-Machado.
 2005. The use of receiver operating
 characteristic curves in biomedical informatics.  <i>Journal of
 Biomedical Informatics</i> <b>38</b>:404â€“-415.</li>

 <li> Manning, Christopher D., Prabhakar Raghavan, and Hinrich
 Sch&#252;tze. 2008. <i> Introduction to Information Retrieval</i>. Cambridge
 University Press.  Chapter 8, Evaluation in information retrieval.</li>
 </ul>
<P>

<P>
<DL>
<DT><B>Since:</B></DT>
  <DD>LingPipe2.1</DD>
<DT><B>Version:</B></DT>
  <DD>4.0.1</DD>
<DT><B>Author:</B></DT>
  <DD>Bob Carpenter, Mike Ross, Breck Baldwin</DD>
</DL>
<HR>

<P>

<!-- ======== CONSTRUCTOR SUMMARY ======== -->

<A NAME="constructor_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Constructor Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#ScoredPrecisionRecallEvaluation()">ScoredPrecisionRecallEvaluation</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Construct a scored precision-recall evaluation.</TD>
</TR>
</TABLE>
&nbsp;
<!-- ========== METHOD SUMMARY =========== -->

<A NAME="method_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Method Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#addCase(boolean, double)">addCase</A></B>(boolean&nbsp;correct,
        double&nbsp;score)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Add a case with the specified correctness and response score.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#addMisses(int)">addMisses</A></B>(int&nbsp;count)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Incrments the positive reference count without adding a
 case from the classifier.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#addNegativeMisses(int)">addNegativeMisses</A></B>(int&nbsp;count)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Incrments the negative reference count without adding a case
 from the classifier.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#areaUnderPrCurve(boolean)">areaUnderPrCurve</A></B>(boolean&nbsp;interpolate)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the area under the curve (AUC) for the recall-precision
 curve with interpolation as specified.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#areaUnderRocCurve(boolean)">areaUnderRocCurve</A></B>(boolean&nbsp;interpolate)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the area under the receiver operating characteristic
 (ROC) curve.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#averagePrecision()">averagePrecision</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the average of precisions at the true positive
 results.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#elevenPtInterpPrecision()">elevenPtInterpPrecision</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the interpolated precision at eleven recall points
 evenly spaced between 0 and 1.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#maximumFMeasure()">maximumFMeasure</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the maximum F<sub><sub>1</sub></sub>-measure for an
 operating point on the PR curve.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#maximumFMeasure(double)">maximumFMeasure</A></B>(double&nbsp;beta)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the maximum F<sub><sub>&beta;</sub></sub>-measure for
 an operating point on the precision-recall curve for a
 specified precision weight <code>&beta; &gt; 0</code>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#numCases()">numCases</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the total number of positive and negative reference
 cases for this evaluation.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#numNegativeRef()">numNegativeRef</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the number of negative reference cases.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#numPositiveRef()">numPositiveRef</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of positive reference cases.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#prBreakevenPoint()">prBreakevenPoint</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double[][]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#prCurve(boolean)">prCurve</A></B>(boolean&nbsp;interpolate)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the precision-recall curve, interpolating if
 the specified flag is true.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#precisionAt(int)">precisionAt</A></B>(int&nbsp;rank)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the precision score achieved by returning the top
 scoring documents up to (but not including) the specified rank.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#printPrecisionRecallCurve(double[][], java.io.PrintWriter)">printPrecisionRecallCurve</A></B>(double[][]&nbsp;prCurve,
                          <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/io/PrintWriter.html?is-external=true" title="class or interface in java.io">PrintWriter</A>&nbsp;pw)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prints a precision-recall curve with F-measures.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>static&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#printScorePrecisionRecallCurve(double[][], java.io.PrintWriter)">printScorePrecisionRecallCurve</A></B>(double[][]&nbsp;prScoreCurve,
                               <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/io/PrintWriter.html?is-external=true" title="class or interface in java.io">PrintWriter</A>&nbsp;pw)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prints a precision-recall curve with score.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double[][]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#prScoreCurve(boolean)">prScoreCurve</A></B>(boolean&nbsp;interpolate)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the array of recall/precision/score operating points
 according to the scores of the cases.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#reciprocalRank()">reciprocalRank</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the reciprocal rank for this evaluation.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double[][]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#rocCurve(boolean)">rocCurve</A></B>(boolean&nbsp;interpolate)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the receiver operating characteristic (ROC) curve for
 the cases ordered by score, interpolating if the specified flag
 is <code>true</code>.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#rPrecision()">rPrecision</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return the R precision.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/String.html?is-external=true" title="class or interface in java.lang">String</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#toString()">toString</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a string-based representation of this scored precision
 recall evaluation.</TD>
</TR>
</TABLE>
&nbsp;<A NAME="methods_inherited_from_class_java.lang.Object"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#EEEEFF" CLASS="TableSubHeadingColor">
<TH ALIGN="left"><B>Methods inherited from class java.lang.<A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">Object</A></B></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#clone()" title="class or interface in java.lang">clone</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#equals(java.lang.Object)" title="class or interface in java.lang">equals</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#finalize()" title="class or interface in java.lang">finalize</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#getClass()" title="class or interface in java.lang">getClass</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#hashCode()" title="class or interface in java.lang">hashCode</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#notify()" title="class or interface in java.lang">notify</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#notifyAll()" title="class or interface in java.lang">notifyAll</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#wait()" title="class or interface in java.lang">wait</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#wait(long)" title="class or interface in java.lang">wait</A>, <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#wait(long, int)" title="class or interface in java.lang">wait</A></CODE></TD>
</TR>
</TABLE>
&nbsp;
<P>

<!-- ========= CONSTRUCTOR DETAIL ======== -->

<A NAME="constructor_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Constructor Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="ScoredPrecisionRecallEvaluation()"><!-- --></A><H3>
ScoredPrecisionRecallEvaluation</H3>
<PRE>
public <B>ScoredPrecisionRecallEvaluation</B>()</PRE>
<DL>
<DD>Construct a scored precision-recall evaluation.
<P>
</DL>

<!-- ============ METHOD DETAIL ========== -->

<A NAME="method_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Method Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="addCase(boolean, double)"><!-- --></A><H3>
addCase</H3>
<PRE>
public void <B>addCase</B>(boolean&nbsp;correct,
                    double&nbsp;score)</PRE>
<DL>
<DD>Add a case with the specified correctness and response score.
 Only positive response cases are considered, and the correct
 flag is set to <code>true</code> if the reference was also
 positive.  The score is just the response score.

 <P><b>Warning:</b> The scores should be sensibly comparable
 across cases.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>correct</CODE> - <code>true</code> if this case was correct.<DD><CODE>score</CODE> - Score of response.</DL>
</DD>
</DL>
<HR>

<A NAME="addMisses(int)"><!-- --></A><H3>
addMisses</H3>
<PRE>
public void <B>addMisses</B>(int&nbsp;count)</PRE>
<DL>
<DD>Incrments the positive reference count without adding a
 case from the classifier. This method is used for
 precision-recall evaluations where the set of returned items
 does not enumerate all positive references.  These misses are
 used in calcuating statistics such as precision-recall curves.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>count</CODE> - Number of outright misses to add to
 this evaluation.
<DT><B>Throws:</B>
<DD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/IllegalArgumentException.html?is-external=true" title="class or interface in java.lang">IllegalArgumentException</A></CODE> - if the count is not positive.</DL>
</DD>
</DL>
<HR>

<A NAME="addNegativeMisses(int)"><!-- --></A><H3>
addNegativeMisses</H3>
<PRE>
public void <B>addNegativeMisses</B>(int&nbsp;count)</PRE>
<DL>
<DD>Incrments the negative reference count without adding a case
 from the classifier. This method is used for ROC evaluations
 where the set of returned items does not enumerate all negative
 references.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>count</CODE> - Number of outright misses to add to
 this evaluation.
<DT><B>Throws:</B>
<DD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/IllegalArgumentException.html?is-external=true" title="class or interface in java.lang">IllegalArgumentException</A></CODE> - if the count is not positive.</DL>
</DD>
</DL>
<HR>

<A NAME="numCases()"><!-- --></A><H3>
numCases</H3>
<PRE>
public int <B>numCases</B>()</PRE>
<DL>
<DD>Returns the total number of positive and negative reference
 cases for this evaluation.  The return value is the sum of
 <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#numPositiveRef()"><CODE>numPositiveRef()</CODE></A> and <code>#numNegativeRef()</code>.
<P>
<DD><DL>

<DT><B>Returns:</B><DD>The number of cases for this evaluation.</DL>
</DD>
</DL>
<HR>

<A NAME="numPositiveRef()"><!-- --></A><H3>
numPositiveRef</H3>
<PRE>
public int <B>numPositiveRef</B>()</PRE>
<DL>
<DD>Returns the number of positive reference cases.  This count
 includes the number of cases added with flag <code>true</code> plus
 the number of misses added.
<P>
<DD><DL>

<DT><B>Returns:</B><DD>Number of positive reference cases.</DL>
</DD>
</DL>
<HR>

<A NAME="numNegativeRef()"><!-- --></A><H3>
numNegativeRef</H3>
<PRE>
public int <B>numNegativeRef</B>()</PRE>
<DL>
<DD>Return the number of negative reference cases.  The count
 includes the number of cases added with flag <code>false</code> plus
 the number of negative misses added.
<P>
<DD><DL>

<DT><B>Returns:</B><DD>Number of negative reference cases.</DL>
</DD>
</DL>
<HR>

<A NAME="rPrecision()"><!-- --></A><H3>
rPrecision</H3>
<PRE>
public double <B>rPrecision</B>()</PRE>
<DL>
<DD>Return the R precision.  See the class documentation above for
 a definition.  The R-precision operating point has identical
 precision and recall by definition.
<P>
<DD><DL>

<DT><B>Returns:</B><DD>The R precision.</DL>
</DD>
</DL>
<HR>

<A NAME="elevenPtInterpPrecision()"><!-- --></A><H3>
elevenPtInterpPrecision</H3>
<PRE>
public double[] <B>elevenPtInterpPrecision</B>()</PRE>
<DL>
<DD>Returns the interpolated precision at eleven recall points
 evenly spaced between 0 and 1.  The recall points are { 0.0,
 0.1, 0.2, ..., 1.0 }.
<P>
<DD><DL>

<DT><B>Returns:</B><DD>Eleven-point interpolated precision.</DL>
</DD>
</DL>
<HR>

<A NAME="averagePrecision()"><!-- --></A><H3>
averagePrecision</H3>
<PRE>
public double <B>averagePrecision</B>()</PRE>
<DL>
<DD>Returns the average of precisions at the true positive
 results.  If an item is missed (i.e., it was added
 by <code>#addMisses(int)</code>, the precision is considered
 to be zero.  (See class documentation for more information.)
<P>
<DD><DL>

<DT><B>Returns:</B><DD>Average precision at each true positive.</DL>
</DD>
</DL>
<HR>

<A NAME="prCurve(boolean)"><!-- --></A><H3>
prCurve</H3>
<PRE>
public double[][] <B>prCurve</B>(boolean&nbsp;interpolate)</PRE>
<DL>
<DD>Returns the precision-recall curve, interpolating if
 the specified flag is true.  See the class documentation
 above for a definition of the curve.

 <p><i>Warning:</i> Despite the name, the values
 returned are in the arrays with recall at index 0
 and precision at index 1.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>interpolate</CODE> - Set to <code>true</code> for precision
 interpolation.
<DT><B>Returns:</B><DD>The precision-recall curve.</DL>
</DD>
</DL>
<HR>

<A NAME="prScoreCurve(boolean)"><!-- --></A><H3>
prScoreCurve</H3>
<PRE>
public double[][] <B>prScoreCurve</B>(boolean&nbsp;interpolate)</PRE>
<DL>
<DD>Returns the array of recall/precision/score operating points
 according to the scores of the cases.  Other than adding
 scores, this method works just like <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#prCurve(boolean)"><CODE>prCurve(boolean)</CODE></A>.
 Index 0 is recall, 1 is precision and 2 is the score.
 <p>.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>interpolate</CODE> - Set to <code>true</code> if the precisions
 are interpolated through pruning dominated points.
<DT><B>Returns:</B><DD>The precision-recall-score curve for the specified category.</DL>
</DD>
</DL>
<HR>

<A NAME="rocCurve(boolean)"><!-- --></A><H3>
rocCurve</H3>
<PRE>
public double[][] <B>rocCurve</B>(boolean&nbsp;interpolate)</PRE>
<DL>
<DD>Returns the receiver operating characteristic (ROC) curve for
 the cases ordered by score, interpolating if the specified flag
 is <code>true</code>.  See the class documentation above for
 a definition and example of the returned curve.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>interpolate</CODE> - Interpolate specificity values.
<DT><B>Returns:</B><DD>The receiver operating characteristic curve.</DL>
</DD>
</DL>
<HR>

<A NAME="maximumFMeasure()"><!-- --></A><H3>
maximumFMeasure</H3>
<PRE>
public double <B>maximumFMeasure</B>()</PRE>
<DL>
<DD>Returns the maximum F<sub><sub>1</sub></sub>-measure for an
 operating point on the PR curve.  See the class documentation
 above for an example and further explanation.
<P>
<DD><DL>

<DT><B>Returns:</B><DD>Maximum f-measure for the specified category.</DL>
</DD>
</DL>
<HR>

<A NAME="maximumFMeasure(double)"><!-- --></A><H3>
maximumFMeasure</H3>
<PRE>
public double <B>maximumFMeasure</B>(double&nbsp;beta)</PRE>
<DL>
<DD>Returns the maximum F<sub><sub>&beta;</sub></sub>-measure for
 an operating point on the precision-recall curve for a
 specified precision weight <code>&beta; &gt; 0</code>.
<P>
<DD><DL>

<DT><B>Returns:</B><DD>Maximum f-measure for the specified category.</DL>
</DD>
</DL>
<HR>

<A NAME="precisionAt(int)"><!-- --></A><H3>
precisionAt</H3>
<PRE>
public double <B>precisionAt</B>(int&nbsp;rank)</PRE>
<DL>
<DD>Returns the precision score achieved by returning the top
 scoring documents up to (but not including) the specified rank.
 The precision-recall curve is not interpolated for this
 computation.  For rank 0, the result <code>Double.NaN</code> is
 returned.
<P>
<DD><DL>

<DT><B>Returns:</B><DD>The precision at the specified rank.</DL>
</DD>
</DL>
<HR>

<A NAME="prBreakevenPoint()"><!-- --></A><H3>
prBreakevenPoint</H3>
<PRE>
public double <B>prBreakevenPoint</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="reciprocalRank()"><!-- --></A><H3>
reciprocalRank</H3>
<PRE>
public double <B>reciprocalRank</B>()</PRE>
<DL>
<DD>Returns the reciprocal rank for this evaluation.  The reciprocal
 rank is defined as the reciprocal <code>1/N</code> of the
 rank <code>N</code> at which the first true positive is found.
 This method counts ranks from 1 rather than 0.

 The return result will be between 1.0 for the first-best result
 being correct and 0.0, for none of the results being correct.
<P>
<DD><DL>

<DT><B>Returns:</B><DD>The reciprocal rank.</DL>
</DD>
</DL>
<HR>

<A NAME="areaUnderPrCurve(boolean)"><!-- --></A><H3>
areaUnderPrCurve</H3>
<PRE>
public double <B>areaUnderPrCurve</B>(boolean&nbsp;interpolate)</PRE>
<DL>
<DD>Returns the area under the curve (AUC) for the recall-precision
 curve with interpolation as specified.  See the class documentation
 for more information.

 <p><b>Warning:</b> This method uses the parallelogram method
 for interpolation rather than the usual interpolation method
 used to calculate AUC for precision-recall in information
 retrieval evaluations.  The usual AUC calculation for PR curves
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>interpolate</CODE> - Set to <code>true</code> to interpolate
 the precision values.
<DT><B>Returns:</B><DD>The area under the specified precision-recall curve.</DL>
</DD>
</DL>
<HR>

<A NAME="areaUnderRocCurve(boolean)"><!-- --></A><H3>
areaUnderRocCurve</H3>
<PRE>
public double <B>areaUnderRocCurve</B>(boolean&nbsp;interpolate)</PRE>
<DL>
<DD>Returns the area under the receiver operating characteristic
 (ROC) curve.  See the class documentation for more information.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>interpolate</CODE> - Set to <code>true</code> to interpolate
 the rejection recall values.
<DT><B>Returns:</B><DD>The area under the ROC curve.</DL>
</DD>
</DL>
<HR>

<A NAME="toString()"><!-- --></A><H3>
toString</H3>
<PRE>
public <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/String.html?is-external=true" title="class or interface in java.lang">String</A> <B>toString</B>()</PRE>
<DL>
<DD>Returns a string-based representation of this scored precision
 recall evaluation.
<P>
<DD><DL>
<DT><B>Overrides:</B><DD><CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true#toString()" title="class or interface in java.lang">toString</A></CODE> in class <CODE><A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Object.html?is-external=true" title="class or interface in java.lang">Object</A></CODE></DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="printPrecisionRecallCurve(double[][], java.io.PrintWriter)"><!-- --></A><H3>
printPrecisionRecallCurve</H3>
<PRE>
public static void <B>printPrecisionRecallCurve</B>(double[][]&nbsp;prCurve,
                                             <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/io/PrintWriter.html?is-external=true" title="class or interface in java.io">PrintWriter</A>&nbsp;pw)</PRE>
<DL>
<DD>Prints a precision-recall curve with F-measures.  The curve is formatted
 as in <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#prCurve(boolean)"><CODE>prCurve(boolean)</CODE></A>: an array of length-2 arrays of doubles.
 In each length-2 array, the recall value is at index 0, and the precision
 is at index 1.  The printed curve prints 3 columns in the following order:
 precision, recall, F-measure.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>prCurve</CODE> - A precision-recall curve.<DD><CODE>pw</CODE> - The output PrintWriter.</DL>
</DD>
</DL>
<HR>

<A NAME="printScorePrecisionRecallCurve(double[][], java.io.PrintWriter)"><!-- --></A><H3>
printScorePrecisionRecallCurve</H3>
<PRE>
public static void <B>printScorePrecisionRecallCurve</B>(double[][]&nbsp;prScoreCurve,
                                                  <A HREF="http://java.sun.com/j2se/1.5.0/docs/api/java/io/PrintWriter.html?is-external=true" title="class or interface in java.io">PrintWriter</A>&nbsp;pw)</PRE>
<DL>
<DD>Prints a precision-recall curve with score.  The curve is formatted
 as in <A HREF="../../../com/aliasi/classify/ScoredPrecisionRecallEvaluation.html#prScoreCurve(boolean)"><CODE>prScoreCurve(boolean)</CODE></A>: an array of length-3 arrays of doubles.
 In each length-3 array, the recall value is at index 0, and the precision
 is at index 1 and score at 2.  The printed curve prints 3 columns in the following order:
 precision, recall, score.
<P>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>prScoreCurve</CODE> - A precision-recall score curve.<DD><CODE>pw</CODE> - The output PrintWriter.</DL>
</DD>
</DL>
<!-- ========= END OF CLASS DATA ========= -->
<HR>


<!-- ======= START OF BOTTOM NAVBAR ====== -->
<A NAME="navbar_bottom"><!-- --></A>
<A HREF="#skip-navbar_bottom" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_bottom_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../index-all.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;<A HREF="../../../com/aliasi/classify/ScoredClassifierEvaluator.html" title="class in com.aliasi.classify"><B>PREV CLASS</B></A>&nbsp;
&nbsp;<A HREF="../../../com/aliasi/classify/TfIdfClassifierTrainer.html" title="class in com.aliasi.classify"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../index.html?com/aliasi/classify/ScoredPrecisionRecallEvaluation.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="ScoredPrecisionRecallEvaluation.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_bottom"></A>
<!-- ======== END OF BOTTOM NAVBAR ======= -->

<HR>

</BODY>
</HTML>
